import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
import math
from collections import deque
import time

# ===== è°ƒè¯•å¼€å…³ =====
DEBUG = True
DEBUG_PRINT_INTERVAL_SEC = 1.0
_last_debug_print_ts = 0.0

# ===== ç–²åŠ³æ£€æµ‹å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====
EAR_THRESHOLD_BASE = 0.21          # åŸºç¡€çœ¼ç›é—­åˆé˜ˆå€¼ï¼ˆå°†è‡ªé€‚åº”è°ƒæ•´ï¼‰
MOUTH_RATIO_THRESHOLD_BASE = 0.32  # åŸºç¡€å˜´å·´å¼€åˆé˜ˆå€¼
CONSEC_FRAMES_EYE = 15             # è¿ç»­é—­çœ¼å¸§æ•°
CONSEC_FRAMES_MOUTH = 25           # è¿ç»­å¼ å˜´å¸§æ•°

# ===== ä¼˜åŒ–å‚æ•° =====
SMOOTH_WINDOW_SIZE = 5             # æ»‘åŠ¨çª—å£å¤§å°ï¼ˆå¹³æ»‘å¤„ç†ï¼‰
ADAPTIVE_THRESHOLD_ALPHA = 0.1     # è‡ªé€‚åº”é˜ˆå€¼æ›´æ–°ç³»æ•°ï¼ˆ0-1ï¼Œè¶Šå°è¶Šç¨³å®šï¼‰
PERCLOS_WINDOW_SECONDS = 60        # PERCLOSæ—¶é—´çª—å£ï¼ˆç§’ï¼‰
BLINK_DURATION_THRESHOLD = 0.2     # çœ¨çœ¼æŒç»­æ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰
MIN_BLINK_INTERVAL = 0.5           # æœ€å°çœ¨çœ¼é—´éš”ï¼ˆç§’ï¼Œé¿å…é‡å¤è®¡æ•°ï¼‰
HEAD_POSE_THRESHOLD = 30           # å¤´éƒ¨å§¿æ€è§’åº¦é˜ˆå€¼ï¼ˆåº¦ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºæ— æ•ˆï¼‰

# ===== çŠ¶æ€å˜é‡ =====
eye_close_count = 0
mouth_open_count = 0
fatigue_alert = False

# ===== ä¼˜åŒ–æ•°æ®ç»“æ„ =====
ear_history = deque(maxlen=SMOOTH_WINDOW_SIZE)      # EARå†å²å€¼ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
mouth_history = deque(maxlen=SMOOTH_WINDOW_SIZE)    # å˜´å·´æ¯”ä¾‹å†å²å€¼
ear_baseline = None                                  # EARåŸºçº¿å€¼ï¼ˆè‡ªé€‚åº”é˜ˆå€¼ï¼‰
mouth_baseline = None                                # å˜´å·´åŸºçº¿å€¼
blink_times = deque(maxlen=100)                     # çœ¨çœ¼æ—¶é—´æˆ³ï¼ˆç”¨äºPERCLOSï¼‰
last_blink_time = 0                                  # ä¸Šæ¬¡çœ¨çœ¼æ—¶é—´
blink_start_time = None                              # å½“å‰çœ¨çœ¼å¼€å§‹æ—¶é—´
perclos_window_start = time.time()                   # PERCLOSçª—å£å¼€å§‹æ—¶é—´

# ===== å…³é”®ç‚¹ç´¢å¼• =====
# ğŸ‘ï¸ çœ¼ç›ï¼ˆå„6ç‚¹ï¼‰
LEFT_EYE_IDXS = [33, 160, 158, 133, 153, 144]
RIGHT_EYE_IDXS = [362, 385, 387, 263, 373, 380]

# ğŸ‘„ å˜´å·´ï¼ˆä»…4ä¸ªæœ€ç¨³å®šç‚¹ç”¨äºå¼€åˆåº¦è®¡ç®—ï¼‰âœ…
MOUTH_RATIO_IDXS = [61, 291, 0, 17]  # å·¦å˜´è§’, å³å˜´è§’, é¼»ä¸‹ç‚¹, ä¸‹å·´é¡¶ç‚¹

# ğŸ–Œï¸ å˜´å·´è½®å»“ï¼ˆ20ç‚¹ï¼Œä»…ç»˜å›¾ç”¨ï¼‰
MOUTH_CONTOUR_IDXS = [
    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291,
    375, 321, 405, 314, 17, 84, 181, 91, 146
]

# ===== åˆå§‹åŒ– FaceLandmarker =====
base_options = python.BaseOptions(model_asset_path='models/face_landmarker.task')
options = vision.FaceLandmarkerOptions(
    base_options=base_options,
    num_faces=1,
    min_face_detection_confidence=0.3,
    min_tracking_confidence=0.3,
)
landmarker = vision.FaceLandmarker.create_from_options(options)

# ===== æ‰“å¼€ RTSP æµ =====
rtsp_url = "rtsp://172.32.0.93/live/0"
cap = cv2.VideoCapture(rtsp_url)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘å»¶è¿Ÿ

print(f"è¿æ¥ RTSP: {rtsp_url}")
print("æŒ‰ 'q' é€€å‡º")

def calculate_ear(eye):
    """è®¡ç®—çœ¼ç›çºµæ¨ªæ¯” EAR"""
    A = math.dist(eye[1], eye[5])
    B = math.dist(eye[2], eye[4])
    C = math.dist(eye[0], eye[3])
    return (A + B) / (2.0 * C) if C != 0 else 0.0

def calculate_mouth_ratio(mouth_pts):
    """
    è®¡ç®—å˜´å·´å¼€åˆåº¦ï¼ˆæ›´é²æ£’ï¼‰
    mouth_pts: [left_corner, right_corner, upper_lip_center, lower_lip_center]
    ä½¿ç”¨å‚ç›´åƒç´ è·ç¦» / å˜´è§’å®½åº¦
    """
    left, right, upper, lower = mouth_pts
    # ä»…ä½¿ç”¨ y åæ ‡å·®ï¼ˆé¿å…æ—‹è½¬/å§¿æ€å½±å“ï¼‰
    mouth_height = abs(lower[1] - upper[1])
    mouth_width = math.dist(left, right)
    return mouth_height / mouth_width if mouth_width > 1e-5 else 0.0

def calculate_head_pose(landmarks):
    """
    è®¡ç®—å¤´éƒ¨å§¿æ€è§’åº¦ï¼ˆpitch, yawï¼‰
    è¿”å›è§’åº¦ï¼ˆåº¦ï¼‰ï¼Œç”¨äºè¿‡æ»¤æ— æ•ˆå¸§
    ä½¿ç”¨é¼»å°–ã€ä¸‹å·´ã€å·¦çœ¼è§’ã€å³çœ¼è§’ç­‰å…³é”®ç‚¹
    """
    if len(landmarks) < 468:  # MediaPipe Face Landmarkeræœ‰468ä¸ªå…³é”®ç‚¹
        return 0.0  # å…³é”®ç‚¹ä¸è¶³ï¼Œè¿”å›0ï¼ˆè§†ä¸ºæœ‰æ•ˆï¼‰
    
    # MediaPipeå…³é”®ç‚¹ç´¢å¼•ï¼ˆåŸºäº468ç‚¹æ¨¡å‹ï¼‰
    # é¼»å°–: 4, ä¸‹å·´: 175, å·¦çœ¼è§’: 33, å³çœ¼è§’: 263
    try:
        nose_tip = landmarks[4]
        chin = landmarks[175] if len(landmarks) > 175 else landmarks[17]
        left_eye = landmarks[33]
        right_eye = landmarks[263]
        
        # è®¡ç®—pitchï¼ˆä¸Šä¸‹ç‚¹å¤´ï¼‰- ä½¿ç”¨é¼»å°–åˆ°ä¸‹å·´çš„å‘é‡
        nose_chin_vec = (chin[0] - nose_tip[0], chin[1] - nose_tip[1])
        pitch = math.degrees(math.atan2(abs(nose_chin_vec[1]), abs(nose_chin_vec[0])))
        
        # è®¡ç®—yawï¼ˆå·¦å³è½¬å¤´ï¼‰- ä½¿ç”¨åŒçœ¼è¿çº¿
        eye_vec = (right_eye[0] - left_eye[0], right_eye[1] - left_eye[1])
        eye_distance = math.sqrt(eye_vec[0]**2 + eye_vec[1]**2)
        if eye_distance > 0:
            # è®¡ç®—çœ¼ç›è¿çº¿çš„è§’åº¦
            yaw = math.degrees(math.atan2(abs(eye_vec[1]), abs(eye_vec[0])))
        else:
            yaw = 0.0
        
        return max(pitch, yaw)  # è¿”å›æœ€å¤§è§’åº¦
    except (IndexError, TypeError):
        return 0.0  # å‡ºé”™æ—¶è¿”å›0ï¼ˆè§†ä¸ºæœ‰æ•ˆï¼Œé¿å…è¯¯åˆ¤ï¼‰

def smooth_value(value, history, window_size=SMOOTH_WINDOW_SIZE):
    """
    æ»‘åŠ¨çª—å£å¹³æ»‘å¤„ç†
    ä½¿ç”¨ç§»åŠ¨å¹³å‡å‡å°‘å™ªå£°
    """
    history.append(value)
    if len(history) < window_size:
        return value  # çª—å£æœªæ»¡æ—¶ç›´æ¥è¿”å›
    return np.mean(list(history))

def update_adaptive_threshold(current_value, baseline, alpha=ADAPTIVE_THRESHOLD_ALPHA):
    """
    æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼
    ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰åŠ¨æ€è°ƒæ•´åŸºçº¿
    """
    if baseline is None:
        return current_value
    return alpha * current_value + (1 - alpha) * baseline

def calculate_perclos(blink_times, window_start_time, window_duration=PERCLOS_WINDOW_SECONDS):
    """
    è®¡ç®—PERCLOSï¼ˆPercentage of Eyelid Closureï¼‰
    åœ¨æ—¶é—´çª—å£å†…çœ¼ç›é—­åˆçš„æ—¶é—´ç™¾åˆ†æ¯”
    """
    current_time = time.time()
    if current_time - window_start_time < window_duration:
        return 0.0
    
    # ç»Ÿè®¡çª—å£å†…çš„çœ¨çœ¼æ¬¡æ•°å’ŒæŒç»­æ—¶é—´
    window_start = current_time - window_duration
    valid_blinks = [bt for bt in blink_times if bt >= window_start]
    
    if len(valid_blinks) < 2:
        return 0.0
    
    # ç®€åŒ–ç‰ˆPERCLOSï¼šåŸºäºçœ¨çœ¼é¢‘ç‡
    blink_frequency = len(valid_blinks) / window_duration
    # æ­£å¸¸çœ¨çœ¼é¢‘ç‡ï¼š15-20æ¬¡/åˆ†é’Ÿï¼Œä½äº10æ¬¡/åˆ†é’Ÿå¯èƒ½ç–²åŠ³
    normal_blink_rate = 0.25  # æ¬¡/ç§’ï¼ˆ15æ¬¡/åˆ†é’Ÿï¼‰
    if blink_frequency < normal_blink_rate * 0.5:  # ä½äºæ­£å¸¸å€¼50%
        return 1.0 - (blink_frequency / normal_blink_rate)
    return 0.0

def detect_blink(ear_value, threshold, current_time):
    """
    æ£€æµ‹çœ¨çœ¼äº‹ä»¶
    è¿”å›æ˜¯å¦æ£€æµ‹åˆ°çœ¨çœ¼
    """
    global last_blink_time, blink_start_time
    
    is_closed = ear_value < threshold
    
    if is_closed:
        if blink_start_time is None:
            blink_start_time = current_time
    else:
        if blink_start_time is not None:
            # çœ¼ç›çå¼€ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ä¸€ä¸ªå®Œæ•´çš„çœ¨çœ¼
            blink_duration = current_time - blink_start_time
            time_since_last = current_time - last_blink_time
            
            # çœ¨çœ¼æŒç»­æ—¶é—´åˆç†ï¼Œä¸”è·ç¦»ä¸Šæ¬¡çœ¨çœ¼è¶³å¤Ÿä¹…
            if (BLINK_DURATION_THRESHOLD * 0.1 <= blink_duration <= BLINK_DURATION_THRESHOLD * 2 and
                time_since_last >= MIN_BLINK_INTERVAL):
                blink_times.append(current_time)
                last_blink_time = current_time
                blink_start_time = None
                return True
            blink_start_time = None
    
    return False

while True:
    ret, frame = cap.read()
    if not ret:
        print("âŒ æ— æ³•è¯»å– RTSP æµï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‘„åƒå¤´")
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
    detection_result = landmarker.detect(mp_image)
    annotated_frame = frame.copy()

    current_ear = 0.0
    current_mouth_ratio = 0.0
    face_found = False
    head_pose_valid = True
    head_pose_angle = 0.0
    current_time = time.time()

    if detection_result.face_landmarks:
        for face_landmarks in detection_result.face_landmarks:
            landmarks = []
            for lm in face_landmarks:
                x = int(lm.x * frame.shape[1])
                y = int(lm.y * frame.shape[0])
                landmarks.append((x, y))

            # æ£€æŸ¥å¤´éƒ¨å§¿æ€ï¼ˆåªå½±å“ç–²åŠ³åˆ¤æ–­ï¼Œä¸å½±å“ç‰¹å¾ç‚¹ç»˜åˆ¶ï¼‰
            head_pose_angle = calculate_head_pose(landmarks)
            head_pose_valid = head_pose_angle < HEAD_POSE_THRESHOLD

            # æå–å…³é”®åŒºåŸŸ
            left_eye = [landmarks[i] for i in LEFT_EYE_IDXS]
            right_eye = [landmarks[i] for i in RIGHT_EYE_IDXS]
            mouth_for_ratio = [landmarks[i] for i in MOUTH_RATIO_IDXS]
            mouth_for_draw = [landmarks[i] for i in MOUTH_CONTOUR_IDXS]

            # è®¡ç®—æŒ‡æ ‡
            left_ear = calculate_ear(left_eye)
            right_ear = calculate_ear(right_eye)
            raw_ear = (left_ear + right_ear) / 2.0
            raw_mouth_ratio = calculate_mouth_ratio(mouth_for_ratio)

            # æ»‘åŠ¨çª—å£å¹³æ»‘å¤„ç†
            current_ear = smooth_value(raw_ear, ear_history)
            current_mouth_ratio = smooth_value(raw_mouth_ratio, mouth_history)

            # æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼ï¼ˆåŸºçº¿ï¼‰
            if ear_baseline is None:
                ear_baseline = current_ear
            else:
                # åªåœ¨çœ¼ç›çå¼€æ—¶æ›´æ–°åŸºçº¿
                if current_ear > ear_baseline * 0.9:  # é¿å…é—­çœ¼æ—¶æ›´æ–°
                    ear_baseline = update_adaptive_threshold(current_ear, ear_baseline)

            if mouth_baseline is None:
                mouth_baseline = current_mouth_ratio
            else:
                # åªåœ¨å˜´å·´é—­åˆæ—¶æ›´æ–°åŸºçº¿
                if current_mouth_ratio < mouth_baseline * 1.1:  # é¿å…å¼ å˜´æ—¶æ›´æ–°
                    mouth_baseline = update_adaptive_threshold(current_mouth_ratio, mouth_baseline)

            # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
            adaptive_ear_threshold = ear_baseline * 0.7 if ear_baseline else EAR_THRESHOLD_BASE
            adaptive_mouth_threshold = mouth_baseline * 1.5 if mouth_baseline else MOUTH_RATIO_THRESHOLD_BASE

            # æ£€æµ‹çœ¨çœ¼
            detect_blink(current_ear, adaptive_ear_threshold, current_time)

            # ç»˜åˆ¶
            cv2.polylines(annotated_frame, [np.array(left_eye, np.int32)], True, (0, 255, 0), 2)
            cv2.polylines(annotated_frame, [np.array(right_eye, np.int32)], True, (0, 255, 0), 2)
            cv2.polylines(annotated_frame, [np.array(mouth_for_draw, np.int32)], True, (0, 0, 255), 2)

            face_found = True
            # åªå¤„ç†ç¬¬ä¸€å¼ è„¸ï¼ˆnum_faces=1ï¼‰ï¼Œé¿å…é‡å¤è¦†ç›–æŒ‡æ ‡/ç»˜åˆ¶
            break
    else:
        face_found = False

    # ===== ä¼˜åŒ–åçš„ç–²åŠ³æ£€æµ‹é€»è¾‘ =====
    perclos_score = 0.0
    fatigue_score = 0.0
    
    if face_found and head_pose_valid:
        # è®¡ç®—PERCLOS
        perclos_score = calculate_perclos(blink_times, perclos_window_start)
        
        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼çš„é—­çœ¼æ£€æµ‹
        adaptive_ear_threshold = ear_baseline * 0.7 if ear_baseline else EAR_THRESHOLD_BASE
        if current_ear < adaptive_ear_threshold:
            eye_close_count += 1
        else:
            eye_close_count = max(0, eye_close_count - 2)

        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼çš„å¼ å˜´æ£€æµ‹
        adaptive_mouth_threshold = mouth_baseline * 1.5 if mouth_baseline else MOUTH_RATIO_THRESHOLD_BASE
        if current_mouth_ratio > adaptive_mouth_threshold:
            mouth_open_count += 1
        else:
            mouth_open_count = max(0, mouth_open_count - 3)

        # å¤šæŒ‡æ ‡èåˆè¯„åˆ†
        eye_score = min(eye_close_count / CONSEC_FRAMES_EYE, 1.0)
        mouth_score = min(mouth_open_count / CONSEC_FRAMES_MOUTH, 1.0)
        perclos_weight = 0.3  # PERCLOSæƒé‡
        eye_weight = 0.5       # é—­çœ¼æƒé‡
        mouth_weight = 0.2     # å¼ å˜´æƒé‡
        
        fatigue_score = (eye_weight * eye_score + 
                        mouth_weight * mouth_score + 
                        perclos_weight * perclos_score)

        # è§¦å‘è­¦å‘Šï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†ï¼‰
        if fatigue_score >= 0.7 or eye_close_count >= CONSEC_FRAMES_EYE or mouth_open_count >= CONSEC_FRAMES_MOUTH:
            fatigue_alert = True
        else:
            fatigue_alert = False
    else:
        # æœªæ£€æµ‹åˆ°äººè„¸æˆ–å¤´éƒ¨å§¿æ€ä¸ä½³æ—¶ï¼Œç¼“æ…¢é‡ç½®è®¡æ•°å™¨
        eye_close_count = max(0, eye_close_count - 1)
        mouth_open_count = max(0, mouth_open_count - 1)
        if not face_found:
            fatigue_alert = False

    # ===== æ˜¾ç¤ºä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====
    y_offset = 30
    line_height = 25

    # åŸºç¡€çŠ¶æ€ï¼ˆæ€»æ˜¯æ˜¾ç¤ºï¼Œä¾¿äºå®šä½æ˜¯å¦æ£€æµ‹åˆ°è„¸ï¼‰
    cv2.putText(
        annotated_frame,
        f"Face: {'YES' if face_found else 'NO'}",
        (10, y_offset + line_height * 9),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.6,
        (0, 255, 0) if face_found else (0, 0, 255),
        2,
    )
    cv2.putText(
        annotated_frame,
        f"PoseAngle: {head_pose_angle:.1f} deg",
        (10, y_offset + line_height * 10),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.6,
        (0, 255, 0) if head_pose_valid else (0, 0, 255),
        2,
    )
    
    # åŸºç¡€æŒ‡æ ‡
    cv2.putText(annotated_frame, f"EAR: {current_ear:.3f}", (10, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    # è‡ªé€‚åº”é˜ˆå€¼æ˜¾ç¤º
    if ear_baseline is not None:
        adaptive_thresh = ear_baseline * 0.7
        cv2.putText(annotated_frame, f"EAR Threshold: {adaptive_thresh:.3f} (Adaptive)", (10, y_offset + line_height),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
    
    cv2.putText(annotated_frame, f"Mouth Ratio: {current_mouth_ratio:.3f}", (10, y_offset + line_height * 2),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    # è®¡æ•°å™¨
    cv2.putText(annotated_frame, f"Eye Close: {eye_close_count}/{CONSEC_FRAMES_EYE}", (10, y_offset + line_height * 3),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    cv2.putText(annotated_frame, f"Mouth Open: {mouth_open_count}/{CONSEC_FRAMES_MOUTH}", (10, y_offset + line_height * 4),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    
    # PERCLOSå’Œç»¼åˆè¯„åˆ†
    if face_found and head_pose_valid:
        cv2.putText(annotated_frame, f"PERCLOS: {perclos_score:.2f}", (10, y_offset + line_height * 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)
        cv2.putText(annotated_frame, f"Fatigue Score: {fatigue_score:.2f}", (10, y_offset + line_height * 6),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)
        cv2.putText(annotated_frame, f"Blinks: {len(blink_times)}", (10, y_offset + line_height * 7),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    # å¤´éƒ¨å§¿æ€çŠ¶æ€
    if face_found:
        status_color = (0, 255, 0) if head_pose_valid else (0, 0, 255)
        status_text = "Head Pose: OK" if head_pose_valid else "Head Pose: Invalid"
        cv2.putText(annotated_frame, status_text, (10, y_offset + line_height * 8),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)

    # ç–²åŠ³è­¦å‘Šï¼ˆæ›´é†’ç›®çš„æ˜¾ç¤ºï¼‰
    if fatigue_alert:
        # åŠé€æ˜çº¢è‰²èƒŒæ™¯
        overlay = annotated_frame.copy()
        cv2.rectangle(overlay, (0, 0), (annotated_frame.shape[1], 100), (0, 0, 255), -1)
        cv2.addWeighted(overlay, 0.3, annotated_frame, 0.7, 0, annotated_frame)
        
        cv2.putText(annotated_frame, "âš ï¸ FATIGUE ALERT! TAKE A BREAK!", 
                    (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)

    cv2.imshow('Fatigue Detection - RTSP (Robust)', annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    # ===== ç»ˆç«¯è°ƒè¯•è¾“å‡ºï¼ˆèŠ‚æµï¼Œé¿å…åˆ·å±ï¼‰ =====
    if DEBUG:
        global _last_debug_print_ts
        if current_time - _last_debug_print_ts >= DEBUG_PRINT_INTERVAL_SEC:
            _last_debug_print_ts = current_time
            num_faces = len(detection_result.face_landmarks) if detection_result.face_landmarks else 0
            adaptive_ear_threshold = ear_baseline * 0.7 if ear_baseline else EAR_THRESHOLD_BASE
            adaptive_mouth_threshold = mouth_baseline * 1.5 if mouth_baseline else MOUTH_RATIO_THRESHOLD_BASE
            print(
                f"[debug] faces={num_faces} face_found={face_found} "
                f"pose_angle={head_pose_angle:.1f} valid={head_pose_valid} "
                f"EAR={current_ear:.3f} thr={adaptive_ear_threshold:.3f} "
                f"Mouth={current_mouth_ratio:.3f} thr={adaptive_mouth_threshold:.3f} "
                f"score={fatigue_score:.2f} alert={fatigue_alert}"
            )

# ===== æ¸…ç† =====
landmarker.close()
cap.release()
cv2.destroyAllWindows()